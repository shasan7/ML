{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b012ec8-3775-469e-a9e6-dbaac87445c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, MSE: 2.0\n",
      "Epoch 2, MSE: 2.0\n",
      "Epoch 3, MSE: 2.6666666666666665\n",
      "Epoch 4, MSE: 2.0\n",
      "Epoch 5, MSE: 2.0\n",
      "Epoch 6, MSE: 2.0\n",
      "Epoch 7, MSE: 2.0\n",
      "Epoch 8, MSE: 2.0\n",
      "Epoch 9, MSE: 2.0\n",
      "Epoch 10, MSE: 2.6666666666666665\n",
      "Epoch 11, MSE: 2.0\n",
      "Epoch 12, MSE: 1.3333333333333333\n",
      "Epoch 13, MSE: 1.3333333333333333\n",
      "Epoch 14, MSE: 2.0\n",
      "Epoch 15, MSE: 1.3333333333333333\n",
      "Epoch 16, MSE: 2.6666666666666665\n",
      "Epoch 17, MSE: 1.3333333333333333\n",
      "Epoch 18, MSE: 1.3333333333333333\n",
      "Epoch 19, MSE: 1.3333333333333333\n",
      "Epoch 20, MSE: 2.0\n",
      "Epoch 21, MSE: 1.3333333333333333\n",
      "Epoch 22, MSE: 1.3333333333333333\n",
      "Epoch 23, MSE: 1.3333333333333333\n",
      "Epoch 24, MSE: 1.3333333333333333\n",
      "Epoch 25, MSE: 1.3333333333333333\n",
      "Epoch 26, MSE: 1.3333333333333333\n",
      "Epoch 27, MSE: 1.3333333333333333\n",
      "Epoch 28, MSE: 1.3333333333333333\n",
      "Epoch 29, MSE: 1.3333333333333333\n",
      "Epoch 30, MSE: 0.0\n",
      "Final weights (input to hidden):\n",
      " [[ 0.69937135  0.64156629 -0.1380775   0.23510462  0.44650026 -0.0761363\n",
      "   0.26322695 -0.83983035 -0.484993   -0.88277817  1.04013615  0.9110535 ]\n",
      " [-0.43527901 -0.01447414 -0.31038937  0.74526968  0.14370657 -0.36037984\n",
      "   0.1493115  -0.65493223  0.68146862  0.06828587  0.57136037 -0.52759503]\n",
      " [-0.4721622   0.51419693 -0.42216018 -0.06876894  0.75296105  0.32656881\n",
      "   0.17051069 -0.10172365 -0.68672213  0.25752541 -0.47911221  0.22648526]\n",
      " [-0.29019105 -0.39362563 -0.33631911 -0.43225636  0.86719904 -0.05988097\n",
      "  -0.38112126 -0.24149703  0.90630465  0.60189251  0.09505624  0.8211344 ]\n",
      " [ 0.95308467 -0.36984727 -0.11916066 -0.81127795 -0.81199399 -0.95325793\n",
      "  -0.71425966  0.73360958  0.02774644  0.2878371  -0.40756427 -0.7797166 ]\n",
      " [ 0.59148105 -0.02993361  0.67122578 -1.04067511 -0.48340756  0.49946413\n",
      "  -0.55340678 -0.61368987  0.47588793 -0.81380874 -0.53040584 -0.0846082 ]]\n",
      "Final weights (hidden to output):\n",
      " [[-0.62070944]\n",
      " [ 0.67560966]\n",
      " [-0.15246865]\n",
      " [-0.55178785]\n",
      " [ 0.16814648]\n",
      " [-0.95155267]\n",
      " [ 0.0714121 ]\n",
      " [ 0.54754646]\n",
      " [-0.30184815]\n",
      " [-0.30013152]\n",
      " [ 0.79557671]\n",
      " [ 0.48643109]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_18992\\2312199949.py:36: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  error_vec[i] = labels[i] - prediction\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def signum(x):\n",
    "    return np.where(x >= 0, 1, -1)\n",
    "\n",
    "# MLP class with one hidden layer\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, learning_rate):\n",
    "        # Initialize weights for input-to-hidden and hidden-to-output layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights_input_hidden = np.random.uniform(-1, 1, (input_size, hidden_size))\n",
    "        self.weights_hidden_output = np.random.uniform(-1, 1, (hidden_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the network\n",
    "        self.hidden_input = np.dot(inputs, self.weights_input_hidden)\n",
    "        self.hidden_output = sigmoid(self.hidden_input)  # Sigmoid activation for hidden layer\n",
    "        \n",
    "        output_layer_input = np.dot(self.hidden_output, self.weights_hidden_output)\n",
    "        output = signum(output_layer_input)  # Signum activation for output layer\n",
    "        return output\n",
    "    \n",
    "    def train(self, inputs, labels, max_epochs=1000):\n",
    "        for epoch in range(max_epochs):\n",
    "            error_vec = np.empty(len(labels))\n",
    "            for i, input_vector in enumerate(inputs):\n",
    "                # Forward pass\n",
    "                prediction = self.forward(input_vector)\n",
    "                error_vec[i] = labels[i] - prediction\n",
    "\n",
    "                # Backpropagation\n",
    "                # Output layer error and weight update\n",
    "                output_error = error_vec[i]\n",
    "                output_delta = output_error  # No derivative needed for signum in output layer\n",
    "                \n",
    "                # Hidden layer error and weight update\n",
    "                hidden_error = output_delta * self.weights_hidden_output[:, 0]\n",
    "                hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
    "                \n",
    "                # Update weights\n",
    "                self.weights_hidden_output += self.learning_rate * output_delta * self.hidden_output[:, np.newaxis]\n",
    "                self.weights_input_hidden += self.learning_rate * input_vector[:, np.newaxis] * hidden_delta\n",
    "            mse = np.mean(np.square(error_vec))\n",
    "            # Print MSE for each epoch\n",
    "            print(f'Epoch {epoch + 1}, MSE: {mse}')\n",
    "            # Stop if MSE is zero\n",
    "            if mse == 0:\n",
    "                break\n",
    "\n",
    "# User-provided values\n",
    "learning_rate = 0.01\n",
    "inputs = np.array([[1, -1, 1, 1, 0, 1], [-1, 1, 0, 1, 1, 0], [1, 1, -1, -1, -1, 0], [1, 0, -1, 0, -1, 1], [-1, 0, 0, 1, 0, -1], [-1, 0, 1, 0, 1, -1]])\n",
    "labels = np.array([1, -1, 1, -1, -1, 1])\n",
    "\n",
    "# Create the MLP\n",
    "mlp = MLP(input_size=len(inputs[0]), hidden_size=2*len(inputs[0]), learning_rate=learning_rate)\n",
    "\n",
    "# Train the MLP\n",
    "mlp.train(inputs, labels)\n",
    "\n",
    "# Final weights\n",
    "print(\"Final weights (input to hidden):\\n\", mlp.weights_input_hidden)\n",
    "print(\"Final weights (hidden to output):\\n\", mlp.weights_hidden_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22513d25-e580-4a9b-a1f7-574773004fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
